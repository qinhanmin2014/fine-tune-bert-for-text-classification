{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdb_tpu_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfbbyhQ-MBs1"
      },
      "source": [
        "import os\r\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qszQHHXVMDRF",
        "outputId": "0f36e1f3-efec-44f8-a719-ca70cbbfd1ce"
      },
      "source": [
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cloud-tpu-client==0.10 in /usr/local/lib/python3.6/dist-packages (0.10)\n",
            "Requirement already satisfied: torch-xla==1.7 from https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (1.7)\n",
            "Requirement already satisfied: google-api-python-client==1.8.0 in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client==0.10) (1.8.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.17.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.16.0)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (4.6)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (51.1.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.12.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.52.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjAVeqBAMkwj",
        "outputId": "f5143f28-1d07-4df7-ac56-fad276d129be"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYwFaQ4-MSFw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a13d9b55-3139-457a-9d50-09ba3e5cbb86"
      },
      "source": [
        "import argparse\r\n",
        "import os\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import time\r\n",
        "from torch.utils.data import TensorDataset, DataLoader\r\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\r\n",
        "import torch_xla.core.xla_model as xm\r\n",
        "import torch_xla.distributed.parallel_loader as pl\r\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:TPU has started up successfully with version pytorch-1.7\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_V2Pd9aMjC2"
      },
      "source": [
        "# can't use max_seq_length=512\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument('-seed', default=0, type=int)\r\n",
        "# parser.add_argument('-max_seq_length', default=512, type=int)\r\n",
        "parser.add_argument('-max_seq_length', default=256, type=int)\r\n",
        "parser.add_argument('-batch_size', default=24, type=int)\r\n",
        "parser.add_argument('-num_epochs', default=4, type=int)\r\n",
        "parser.add_argument('-learning_rate', default=2e-5, type=float)\r\n",
        "parser.add_argument('-max_grad_norm', default=1.0, type=float)\r\n",
        "parser.add_argument('-warm_up_proportion', default=0.1, type=float)\r\n",
        "parser.add_argument('-bert_path', default='bert-base-uncased')\r\n",
        "parser.add_argument('-trunc_mode', default=128, type=str)\r\n",
        "args = parser.parse_args([])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AGDHjJxMoyy",
        "outputId": "4b1b2d9e-3c64-4ed8-90b3-46d8077e7c3c"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(args.bert_path)\r\n",
        "wrapped_model = xmp.MpModelWrapper(BertForSequenceClassification.from_pretrained(args.bert_path, num_labels=2))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EPTsvzkMqzp",
        "outputId": "589c8e40-294a-43d9-9417-df9b967f7df5"
      },
      "source": [
        "def load_data(path):\r\n",
        "    input_ids, attention_mask, token_type_ids = [], [], []\r\n",
        "    sentiments = []\r\n",
        "    input_file = open(path, encoding=\"utf8\")\r\n",
        "    lines = input_file.readlines()\r\n",
        "    input_file.close()\r\n",
        "    for line in tqdm(lines):\r\n",
        "        label, text = line.split(\"\\t\")\r\n",
        "        text = tokenizer.tokenize(text)\r\n",
        "        if args.trunc_mode == \"head\":\r\n",
        "            if len(text) > args.max_seq_length - 2:\r\n",
        "                text = text[:args.max_seq_length - 2]\r\n",
        "        elif args.trunc_mode == \"tail\":\r\n",
        "            if len(text) > args.max_seq_length - 2:\r\n",
        "                text = text[-(args.max_seq_length - 2):]\r\n",
        "        else:\r\n",
        "            args.trunc_mode = int(args.trunc_mode)\r\n",
        "            assert args.trunc_mode < args.max_seq_length\r\n",
        "            if len(text) > args.max_seq_length - 2:\r\n",
        "                text = text[:args.trunc_mode] + text[-(args.max_seq_length - 2 - args.trunc_mode):]\r\n",
        "        text = [\"[CLS]\"] + text + [\"[SEP]\"]\r\n",
        "        attention_mask.append([1] * len(text) + [0] * (args.max_seq_length - len(text)))\r\n",
        "        token_type_ids.append([0] * args.max_seq_length)\r\n",
        "        input_ids.append(tokenizer.convert_tokens_to_ids(text) + [0] * (args.max_seq_length - len(text)))\r\n",
        "        sentiments.append(int(label))\r\n",
        "    return np.array(input_ids), np.array(attention_mask), np.array(token_type_ids), np.array(sentiments)\r\n",
        "\r\n",
        "train_input_ids, train_attention_mask, train_token_type_ids, y_train = load_data('train.csv')\r\n",
        "test_input_ids, test_attention_mask, test_token_type_ids, y_test = load_data('test.csv')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 25000/25000 [02:05<00:00, 198.70it/s]\n",
            "100%|██████████| 25000/25000 [02:07<00:00, 195.86it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxf2MqxsMsvC"
      },
      "source": [
        "train_input_ids = torch.tensor(train_input_ids, dtype=torch.long)\r\n",
        "train_attention_mask = torch.tensor(train_attention_mask, dtype=torch.float)\r\n",
        "train_token_type_ids = torch.tensor(train_token_type_ids, dtype=torch.long)\r\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\r\n",
        "test_input_ids = torch.tensor(test_input_ids, dtype=torch.long)\r\n",
        "test_attention_mask = torch.tensor(test_attention_mask, dtype=torch.float)\r\n",
        "test_token_type_ids = torch.tensor(test_token_type_ids, dtype=torch.long)\r\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\r\n",
        "train_data = TensorDataset(train_input_ids, train_attention_mask, train_token_type_ids, y_train)\r\n",
        "train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True)\r\n",
        "test_data = TensorDataset(test_input_ids, test_attention_mask, test_token_type_ids, y_test)\r\n",
        "test_loader = DataLoader(test_data, batch_size=args.batch_size, shuffle=False)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9JurQgyMw6M"
      },
      "source": [
        "def _mp_fn(index, flags):\r\n",
        "    torch.manual_seed(args.seed)\r\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\r\n",
        "        train_data,\r\n",
        "        num_replicas=xm.xrt_world_size(),\r\n",
        "        rank=xm.get_ordinal(),\r\n",
        "        shuffle=True)\r\n",
        "    test_sampler = torch.utils.data.distributed.DistributedSampler(\r\n",
        "        test_data,\r\n",
        "        num_replicas=xm.xrt_world_size(),\r\n",
        "        rank=xm.get_ordinal(),\r\n",
        "        shuffle=False)\r\n",
        "    train_loader = torch.utils.data.DataLoader(\r\n",
        "        train_data,\r\n",
        "        batch_size=args.batch_size,\r\n",
        "        sampler=train_sampler,\r\n",
        "        num_workers=4,\r\n",
        "        drop_last=True)\r\n",
        "    test_loader = torch.utils.data.DataLoader(\r\n",
        "        test_data,\r\n",
        "        batch_size=args.batch_size,\r\n",
        "        sampler=test_sampler,\r\n",
        "        num_workers=4,\r\n",
        "        drop_last=False)\r\n",
        "    device = xm.xla_device()\r\n",
        "    model = wrapped_model.to(device)\r\n",
        "    param_optimizer = list(model.named_parameters())\r\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\r\n",
        "    optimizer_grouped_parameters = [\r\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\r\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\r\n",
        "        ]\r\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, correct_bias=False)\r\n",
        "    scheduler = get_linear_schedule_with_warmup(\r\n",
        "            optimizer, num_warmup_steps=len(train_loader) * args.num_epochs * args.warm_up_proportion,\r\n",
        "            num_training_steps=len(train_loader) * args.num_epochs)\r\n",
        "    total_step = len(train_loader)\r\n",
        "    for epoch in range(args.num_epochs):\r\n",
        "        model.train()\r\n",
        "        para_train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\r\n",
        "        for i, (cur_input_ids, cur_attention_mask, cur_token_type_ids, cur_y) in enumerate(para_train_loader):\r\n",
        "            cur_input_ids = cur_input_ids.to(device)\r\n",
        "            cur_attention_mask = cur_attention_mask.to(device)\r\n",
        "            cur_token_type_ids = cur_token_type_ids.to(device)\r\n",
        "            cur_y = cur_y.to(device)\r\n",
        "            outputs = model(cur_input_ids, cur_attention_mask, cur_token_type_ids)\r\n",
        "            loss = nn.CrossEntropyLoss()(outputs[0], cur_y)\r\n",
        "            optimizer.zero_grad()\r\n",
        "            loss.backward()\r\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\r\n",
        "            xm.optimizer_step(optimizer)\r\n",
        "            scheduler.step()\r\n",
        "            if (i + 1) % 10 == 0:\r\n",
        "                print ('[{}] [xla:{}] Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\r\n",
        "                        time.strftime(\"%Y-%m-%d %H:%M:%S\"), xm.get_ordinal(), epoch + 1,\r\n",
        "                        args.num_epochs, i + 1, total_step, loss.item()))\r\n",
        "        xm.master_print(\"Finished training epoch {}\".format(epoch))\r\n",
        "        model.eval()\r\n",
        "        with torch.no_grad():\r\n",
        "            correct = 0\r\n",
        "            total = 0\r\n",
        "            para_test_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\r\n",
        "            for i, (cur_input_ids, cur_attention_mask, cur_token_type_ids, cur_y) in enumerate(para_test_loader):\r\n",
        "                cur_input_ids = cur_input_ids.to(device)\r\n",
        "                cur_attention_mask = cur_attention_mask.to(device)\r\n",
        "                cur_token_type_ids = cur_token_type_ids.to(device)\r\n",
        "                cur_y = cur_y.to(device)\r\n",
        "                outputs = model(cur_input_ids, cur_attention_mask, cur_token_type_ids)\r\n",
        "                _, predicted = torch.max(outputs[0], 1)\r\n",
        "                total += cur_y.size(0)\r\n",
        "                correct += (predicted == cur_y).sum().item()\r\n",
        "            accuracy = correct / total\r\n",
        "            print ('[{}] [xla:{}] samples: {} accuracy: {}'.format(\r\n",
        "                    time.strftime(\"%Y-%m-%d %H:%M:%S\"), xm.get_ordinal(), total, accuracy))\r\n",
        "            acc_reduced = xm.mesh_reduce('acc_reduce', accuracy, lambda x: sum(x) / len(x))\r\n",
        "            xm.master_print('reduced accuracy: {}'.format(acc_reduced))\r\n",
        "        xm.master_print(\"Finished evaluating epoch {}\".format(epoch))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A65E2vcJM2kT",
        "outputId": "22adb60d-3b1a-4dbd-cdbe-a527780a121c"
      },
      "source": [
        "FLAGS={}\r\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-01-15 12:59:20] [xla:5] Epoch [1/4], Step [10/130], Loss: 0.6708\n",
            "[2021-01-15 12:59:21] [xla:1] Epoch [1/4], Step [10/130], Loss: 0.6749\n",
            "[2021-01-15 12:59:20] [xla:6] Epoch [1/4], Step [10/130], Loss: 0.6633\n",
            "[2021-01-15 12:59:21] [xla:4] Epoch [1/4], Step [10/130], Loss: 0.6430\n",
            "[2021-01-15 12:59:21] [xla:7] Epoch [1/4], Step [10/130], Loss: 0.6497\n",
            "[2021-01-15 12:59:21] [xla:0] Epoch [1/4], Step [10/130], Loss: 0.6531\n",
            "[2021-01-15 12:59:21] [xla:2] Epoch [1/4], Step [10/130], Loss: 0.6735\n",
            "[2021-01-15 12:59:21] [xla:3] Epoch [1/4], Step [10/130], Loss: 0.6801\n",
            "[2021-01-15 12:59:31] [xla:2] Epoch [1/4], Step [20/130], Loss: 0.3988\n",
            "[2021-01-15 12:59:31] [xla:4] Epoch [1/4], Step [20/130], Loss: 0.4349\n",
            "[2021-01-15 12:59:31] [xla:1] Epoch [1/4], Step [20/130], Loss: 0.4195\n",
            "[2021-01-15 12:59:31] [xla:3] Epoch [1/4], Step [20/130], Loss: 0.4930\n",
            "[2021-01-15 12:59:31] [xla:7] Epoch [1/4], Step [20/130], Loss: 0.4669\n",
            "[2021-01-15 12:59:31] [xla:6] Epoch [1/4], Step [20/130], Loss: 0.3244\n",
            "[2021-01-15 12:59:31] [xla:5] Epoch [1/4], Step [20/130], Loss: 0.4813\n",
            "[2021-01-15 12:59:31] [xla:0] Epoch [1/4], Step [20/130], Loss: 0.3883\n",
            "[2021-01-15 12:59:42] [xla:1] Epoch [1/4], Step [30/130], Loss: 0.0937\n",
            "[2021-01-15 12:59:42] [xla:7] Epoch [1/4], Step [30/130], Loss: 0.3413\n",
            "[2021-01-15 12:59:42] [xla:5] Epoch [1/4], Step [30/130], Loss: 0.5295\n",
            "[2021-01-15 12:59:42] [xla:2] Epoch [1/4], Step [30/130], Loss: 0.2487\n",
            "[2021-01-15 12:59:42] [xla:4] Epoch [1/4], Step [30/130], Loss: 0.4236\n",
            "[2021-01-15 12:59:42] [xla:3] Epoch [1/4], Step [30/130], Loss: 0.4451\n",
            "[2021-01-15 12:59:42] [xla:6] Epoch [1/4], Step [30/130], Loss: 0.4741\n",
            "[2021-01-15 12:59:42] [xla:0] Epoch [1/4], Step [30/130], Loss: 0.0959\n",
            "[2021-01-15 12:59:53] [xla:3] Epoch [1/4], Step [40/130], Loss: 0.3881\n",
            "[2021-01-15 12:59:53] [xla:4] Epoch [1/4], Step [40/130], Loss: 0.4118\n",
            "[2021-01-15 12:59:53] [xla:7] Epoch [1/4], Step [40/130], Loss: 0.3020\n",
            "[2021-01-15 12:59:53] [xla:0] Epoch [1/4], Step [40/130], Loss: 0.1522\n",
            "[2021-01-15 12:59:53] [xla:5] Epoch [1/4], Step [40/130], Loss: 0.2185\n",
            "[2021-01-15 12:59:53] [xla:6] Epoch [1/4], Step [40/130], Loss: 0.4676\n",
            "[2021-01-15 12:59:53] [xla:2] Epoch [1/4], Step [40/130], Loss: 0.2233\n",
            "[2021-01-15 12:59:53] [xla:1] Epoch [1/4], Step [40/130], Loss: 0.3108\n",
            "[2021-01-15 13:00:04] [xla:6] Epoch [1/4], Step [50/130], Loss: 0.4908\n",
            "[2021-01-15 13:00:04] [xla:2] Epoch [1/4], Step [50/130], Loss: 0.2162\n",
            "[2021-01-15 13:00:04] [xla:5] Epoch [1/4], Step [50/130], Loss: 0.2797\n",
            "[2021-01-15 13:00:04] [xla:3] Epoch [1/4], Step [50/130], Loss: 0.2687\n",
            "[2021-01-15 13:00:04] [xla:4] Epoch [1/4], Step [50/130], Loss: 0.3662\n",
            "[2021-01-15 13:00:04] [xla:0] Epoch [1/4], Step [50/130], Loss: 0.2918\n",
            "[2021-01-15 13:00:04] [xla:1] Epoch [1/4], Step [50/130], Loss: 0.1185\n",
            "[2021-01-15 13:00:04] [xla:7] Epoch [1/4], Step [50/130], Loss: 0.1656\n",
            "[2021-01-15 13:00:15] [xla:2] Epoch [1/4], Step [60/130], Loss: 0.3869\n",
            "[2021-01-15 13:00:15] [xla:5] Epoch [1/4], Step [60/130], Loss: 0.5442\n",
            "[2021-01-15 13:00:15] [xla:6] Epoch [1/4], Step [60/130], Loss: 0.1587\n",
            "[2021-01-15 13:00:15] [xla:4] Epoch [1/4], Step [60/130], Loss: 0.5334\n",
            "[2021-01-15 13:00:15] [xla:1] Epoch [1/4], Step [60/130], Loss: 0.5631\n",
            "[2021-01-15 13:00:15] [xla:7] Epoch [1/4], Step [60/130], Loss: 0.4009\n",
            "[2021-01-15 13:00:15] [xla:3] Epoch [1/4], Step [60/130], Loss: 0.7524\n",
            "[2021-01-15 13:00:15] [xla:0] Epoch [1/4], Step [60/130], Loss: 0.2750\n",
            "[2021-01-15 13:00:26] [xla:5] Epoch [1/4], Step [70/130], Loss: 0.2942\n",
            "[2021-01-15 13:00:26] [xla:7] Epoch [1/4], Step [70/130], Loss: 0.2936\n",
            "[2021-01-15 13:00:26] [xla:2] Epoch [1/4], Step [70/130], Loss: 0.2659\n",
            "[2021-01-15 13:00:26] [xla:1] Epoch [1/4], Step [70/130], Loss: 0.2988\n",
            "[2021-01-15 13:00:26] [xla:0] Epoch [1/4], Step [70/130], Loss: 0.4929\n",
            "[2021-01-15 13:00:26] [xla:4] Epoch [1/4], Step [70/130], Loss: 0.1768\n",
            "[2021-01-15 13:00:26] [xla:6] Epoch [1/4], Step [70/130], Loss: 0.1983\n",
            "[2021-01-15 13:00:26] [xla:3] Epoch [1/4], Step [70/130], Loss: 0.4352\n",
            "[2021-01-15 13:00:37] [xla:6] Epoch [1/4], Step [80/130], Loss: 0.3678\n",
            "[2021-01-15 13:00:37] [xla:7] Epoch [1/4], Step [80/130], Loss: 0.0803\n",
            "[2021-01-15 13:00:37] [xla:1] Epoch [1/4], Step [80/130], Loss: 0.6307\n",
            "[2021-01-15 13:00:37] [xla:3] Epoch [1/4], Step [80/130], Loss: 0.3439\n",
            "[2021-01-15 13:00:37] [xla:4] Epoch [1/4], Step [80/130], Loss: 0.2842\n",
            "[2021-01-15 13:00:37] [xla:0] Epoch [1/4], Step [80/130], Loss: 0.3206\n",
            "[2021-01-15 13:00:37] [xla:5] Epoch [1/4], Step [80/130], Loss: 0.1299\n",
            "[2021-01-15 13:00:37] [xla:2] Epoch [1/4], Step [80/130], Loss: 0.1494\n",
            "[2021-01-15 13:00:48] [xla:5] Epoch [1/4], Step [90/130], Loss: 0.3991\n",
            "[2021-01-15 13:00:48] [xla:6] Epoch [1/4], Step [90/130], Loss: 0.3956\n",
            "[2021-01-15 13:00:48] [xla:4] Epoch [1/4], Step [90/130], Loss: 0.1627\n",
            "[2021-01-15 13:00:48] [xla:3] Epoch [1/4], Step [90/130], Loss: 0.2180\n",
            "[2021-01-15 13:00:48] [xla:7] Epoch [1/4], Step [90/130], Loss: 0.1056\n",
            "[2021-01-15 13:00:48] [xla:1] Epoch [1/4], Step [90/130], Loss: 0.1483\n",
            "[2021-01-15 13:00:48] [xla:0] Epoch [1/4], Step [90/130], Loss: 0.1012\n",
            "[2021-01-15 13:00:48] [xla:2] Epoch [1/4], Step [90/130], Loss: 0.3055\n",
            "[2021-01-15 13:00:59] [xla:0] Epoch [1/4], Step [100/130], Loss: 0.0802\n",
            "[2021-01-15 13:00:59] [xla:5] Epoch [1/4], Step [100/130], Loss: 0.2257\n",
            "[2021-01-15 13:00:59] [xla:1] Epoch [1/4], Step [100/130], Loss: 0.1078\n",
            "[2021-01-15 13:00:59] [xla:7] Epoch [1/4], Step [100/130], Loss: 0.2855\n",
            "[2021-01-15 13:00:59] [xla:4] Epoch [1/4], Step [100/130], Loss: 0.5568\n",
            "[2021-01-15 13:00:59] [xla:3] Epoch [1/4], Step [100/130], Loss: 0.1567\n",
            "[2021-01-15 13:00:59] [xla:6] Epoch [1/4], Step [100/130], Loss: 0.0708\n",
            "[2021-01-15 13:00:59] [xla:2] Epoch [1/4], Step [100/130], Loss: 0.0707\n",
            "[2021-01-15 13:01:10] [xla:5] Epoch [1/4], Step [110/130], Loss: 0.3020\n",
            "[2021-01-15 13:01:10] [xla:7] Epoch [1/4], Step [110/130], Loss: 0.1948\n",
            "[2021-01-15 13:01:10] [xla:6] Epoch [1/4], Step [110/130], Loss: 0.2651\n",
            "[2021-01-15 13:01:10] [xla:2] Epoch [1/4], Step [110/130], Loss: 0.2141\n",
            "[2021-01-15 13:01:10] [xla:4] Epoch [1/4], Step [110/130], Loss: 0.2598\n",
            "[2021-01-15 13:01:10] [xla:1] Epoch [1/4], Step [110/130], Loss: 0.2238\n",
            "[2021-01-15 13:01:10] [xla:3] Epoch [1/4], Step [110/130], Loss: 0.1531\n",
            "[2021-01-15 13:01:10] [xla:0] Epoch [1/4], Step [110/130], Loss: 0.2521\n",
            "[2021-01-15 13:01:21] [xla:2] Epoch [1/4], Step [120/130], Loss: 0.2547\n",
            "[2021-01-15 13:01:21] [xla:5] Epoch [1/4], Step [120/130], Loss: 0.1990\n",
            "[2021-01-15 13:01:21] [xla:3] Epoch [1/4], Step [120/130], Loss: 0.2862\n",
            "[2021-01-15 13:01:21] [xla:0] Epoch [1/4], Step [120/130], Loss: 0.1719\n",
            "[2021-01-15 13:01:21] [xla:6] Epoch [1/4], Step [120/130], Loss: 0.2456\n",
            "[2021-01-15 13:01:21] [xla:4] Epoch [1/4], Step [120/130], Loss: 0.2160\n",
            "[2021-01-15 13:01:21] [xla:7] Epoch [1/4], Step [120/130], Loss: 0.1097\n",
            "[2021-01-15 13:01:21] [xla:1] Epoch [1/4], Step [120/130], Loss: 0.1353\n",
            "[2021-01-15 13:01:32] [xla:1] Epoch [1/4], Step [130/130], Loss: 0.1567\n",
            "[2021-01-15 13:01:32] [xla:2] Epoch [1/4], Step [130/130], Loss: 0.2972\n",
            "[2021-01-15 13:01:32] [xla:6] Epoch [1/4], Step [130/130], Loss: 0.1518\n",
            "[2021-01-15 13:01:32] [xla:5] Epoch [1/4], Step [130/130], Loss: 0.1345\n",
            "[2021-01-15 13:01:32] [xla:7] Epoch [1/4], Step [130/130], Loss: 0.1938\n",
            "[2021-01-15 13:01:32] [xla:3] Epoch [1/4], Step [130/130], Loss: 0.2073\n",
            "[2021-01-15 13:01:32] [xla:0] Epoch [1/4], Step [130/130], Loss: 0.4771\n",
            "[2021-01-15 13:01:32] [xla:4] Epoch [1/4], Step [130/130], Loss: 0.1336\n",
            "Finished training epoch 0\n",
            "[2021-01-15 13:01:58] [xla:7] samples: 3125 accuracy: 0.92064\n",
            "[2021-01-15 13:01:59] [xla:5] samples: 3125 accuracy: 0.93248\n",
            "[2021-01-15 13:01:59] [xla:0] samples: 3125 accuracy: 0.91968\n",
            "[2021-01-15 13:01:59] [xla:2] samples: 3125 accuracy: 0.93408\n",
            "[2021-01-15 13:01:59] [xla:6] samples: 3125 accuracy: 0.93376\n",
            "[2021-01-15 13:01:59] [xla:1] samples: 3125 accuracy: 0.93248\n",
            "[2021-01-15 13:01:59] [xla:4] samples: 3125 accuracy: 0.92992\n",
            "[2021-01-15 13:01:59] [xla:3] samples: 3125 accuracy: 0.93504\n",
            "reduced accuracy: 0.92976\n",
            "Finished evaluating epoch 0\n",
            "[2021-01-15 13:02:11] [xla:5] Epoch [2/4], Step [10/130], Loss: 0.0620\n",
            "[2021-01-15 13:02:11] [xla:2] Epoch [2/4], Step [10/130], Loss: 0.1885\n",
            "[2021-01-15 13:02:11] [xla:4] Epoch [2/4], Step [10/130], Loss: 0.0253\n",
            "[2021-01-15 13:02:11] [xla:7] Epoch [2/4], Step [10/130], Loss: 0.1584\n",
            "[2021-01-15 13:02:11] [xla:6] Epoch [2/4], Step [10/130], Loss: 0.0496\n",
            "[2021-01-15 13:02:11] [xla:3] Epoch [2/4], Step [10/130], Loss: 0.3314\n",
            "[2021-01-15 13:02:11] [xla:0] Epoch [2/4], Step [10/130], Loss: 0.1414\n",
            "[2021-01-15 13:02:11] [xla:1] Epoch [2/4], Step [10/130], Loss: 0.1221\n",
            "[2021-01-15 13:02:22] [xla:5] Epoch [2/4], Step [20/130], Loss: 0.3764\n",
            "[2021-01-15 13:02:22] [xla:7] Epoch [2/4], Step [20/130], Loss: 0.1999\n",
            "[2021-01-15 13:02:22] [xla:1] Epoch [2/4], Step [20/130], Loss: 0.2496\n",
            "[2021-01-15 13:02:22] [xla:2] Epoch [2/4], Step [20/130], Loss: 0.0598\n",
            "[2021-01-15 13:02:22] [xla:4] Epoch [2/4], Step [20/130], Loss: 0.1282\n",
            "[2021-01-15 13:02:22] [xla:6] Epoch [2/4], Step [20/130], Loss: 0.0229\n",
            "[2021-01-15 13:02:22] [xla:3] Epoch [2/4], Step [20/130], Loss: 0.1634\n",
            "[2021-01-15 13:02:22] [xla:0] Epoch [2/4], Step [20/130], Loss: 0.0936\n",
            "[2021-01-15 13:02:33] [xla:0] Epoch [2/4], Step [30/130], Loss: 0.0309\n",
            "[2021-01-15 13:02:33] [xla:4] Epoch [2/4], Step [30/130], Loss: 0.2195\n",
            "[2021-01-15 13:02:33] [xla:3] Epoch [2/4], Step [30/130], Loss: 0.3377\n",
            "[2021-01-15 13:02:33] [xla:5] Epoch [2/4], Step [30/130], Loss: 0.1172\n",
            "[2021-01-15 13:02:33] [xla:6] Epoch [2/4], Step [30/130], Loss: 0.4407\n",
            "[2021-01-15 13:02:33] [xla:2] Epoch [2/4], Step [30/130], Loss: 0.0278\n",
            "[2021-01-15 13:02:33] [xla:7] Epoch [2/4], Step [30/130], Loss: 0.2069\n",
            "[2021-01-15 13:02:33] [xla:1] Epoch [2/4], Step [30/130], Loss: 0.0136\n",
            "[2021-01-15 13:02:44] [xla:5] Epoch [2/4], Step [40/130], Loss: 0.0197\n",
            "[2021-01-15 13:02:44] [xla:3] Epoch [2/4], Step [40/130], Loss: 0.1575\n",
            "[2021-01-15 13:02:44] [xla:6] Epoch [2/4], Step [40/130], Loss: 0.4499\n",
            "[2021-01-15 13:02:44] [xla:4] Epoch [2/4], Step [40/130], Loss: 0.3811\n",
            "[2021-01-15 13:02:44] [xla:1] Epoch [2/4], Step [40/130], Loss: 0.1424\n",
            "[2021-01-15 13:02:44] [xla:0] Epoch [2/4], Step [40/130], Loss: 0.0224\n",
            "[2021-01-15 13:02:44] [xla:7] Epoch [2/4], Step [40/130], Loss: 0.3582\n",
            "[2021-01-15 13:02:44] [xla:2] Epoch [2/4], Step [40/130], Loss: 0.0355\n",
            "[2021-01-15 13:02:55] [xla:0] Epoch [2/4], Step [50/130], Loss: 0.0269\n",
            "[2021-01-15 13:02:55] [xla:2] Epoch [2/4], Step [50/130], Loss: 0.0190\n",
            "[2021-01-15 13:02:55] [xla:4] Epoch [2/4], Step [50/130], Loss: 0.1852\n",
            "[2021-01-15 13:02:55] [xla:3] Epoch [2/4], Step [50/130], Loss: 0.1731\n",
            "[2021-01-15 13:02:55] [xla:7] Epoch [2/4], Step [50/130], Loss: 0.0130\n",
            "[2021-01-15 13:02:55] [xla:6] Epoch [2/4], Step [50/130], Loss: 0.2103\n",
            "[2021-01-15 13:02:55] [xla:5] Epoch [2/4], Step [50/130], Loss: 0.0120\n",
            "[2021-01-15 13:02:55] [xla:1] Epoch [2/4], Step [50/130], Loss: 0.0518\n",
            "[2021-01-15 13:03:06] [xla:6] Epoch [2/4], Step [60/130], Loss: 0.1307\n",
            "[2021-01-15 13:03:06] [xla:4] Epoch [2/4], Step [60/130], Loss: 0.0782\n",
            "[2021-01-15 13:03:06] [xla:5] Epoch [2/4], Step [60/130], Loss: 0.2421\n",
            "[2021-01-15 13:03:06] [xla:3] Epoch [2/4], Step [60/130], Loss: 0.4881\n",
            "[2021-01-15 13:03:06] [xla:0] Epoch [2/4], Step [60/130], Loss: 0.1907\n",
            "[2021-01-15 13:03:06] [xla:2] Epoch [2/4], Step [60/130], Loss: 0.4099\n",
            "[2021-01-15 13:03:06] [xla:7] Epoch [2/4], Step [60/130], Loss: 0.1527\n",
            "[2021-01-15 13:03:06] [xla:1] Epoch [2/4], Step [60/130], Loss: 0.6217\n",
            "[2021-01-15 13:03:17] [xla:3] Epoch [2/4], Step [70/130], Loss: 0.1905\n",
            "[2021-01-15 13:03:17] [xla:5] Epoch [2/4], Step [70/130], Loss: 0.2425\n",
            "[2021-01-15 13:03:17] [xla:4] Epoch [2/4], Step [70/130], Loss: 0.0260\n",
            "[2021-01-15 13:03:17] [xla:0] Epoch [2/4], Step [70/130], Loss: 0.4584\n",
            "[2021-01-15 13:03:17] [xla:6] Epoch [2/4], Step [70/130], Loss: 0.0779\n",
            "[2021-01-15 13:03:17] [xla:1] Epoch [2/4], Step [70/130], Loss: 0.1317\n",
            "[2021-01-15 13:03:17] [xla:2] Epoch [2/4], Step [70/130], Loss: 0.0379\n",
            "[2021-01-15 13:03:17] [xla:7] Epoch [2/4], Step [70/130], Loss: 0.1022\n",
            "[2021-01-15 13:03:28] [xla:7] Epoch [2/4], Step [80/130], Loss: 0.0209\n",
            "[2021-01-15 13:03:28] [xla:0] Epoch [2/4], Step [80/130], Loss: 0.2572\n",
            "[2021-01-15 13:03:28] [xla:6] Epoch [2/4], Step [80/130], Loss: 0.1785\n",
            "[2021-01-15 13:03:28] [xla:1] Epoch [2/4], Step [80/130], Loss: 0.2489\n",
            "[2021-01-15 13:03:28] [xla:2] Epoch [2/4], Step [80/130], Loss: 0.1544\n",
            "[2021-01-15 13:03:28] [xla:4] Epoch [2/4], Step [80/130], Loss: 0.1021\n",
            "[2021-01-15 13:03:28] [xla:5] Epoch [2/4], Step [80/130], Loss: 0.2263\n",
            "[2021-01-15 13:03:28] [xla:3] Epoch [2/4], Step [80/130], Loss: 0.3105\n",
            "[2021-01-15 13:03:39] [xla:5] Epoch [2/4], Step [90/130], Loss: 0.0960\n",
            "[2021-01-15 13:03:39] [xla:6] Epoch [2/4], Step [90/130], Loss: 0.5460\n",
            "[2021-01-15 13:03:39] [xla:2] Epoch [2/4], Step [90/130], Loss: 0.2134\n",
            "[2021-01-15 13:03:39] [xla:1] Epoch [2/4], Step [90/130], Loss: 0.0440\n",
            "[2021-01-15 13:03:39] [xla:7] Epoch [2/4], Step [90/130], Loss: 0.1909\n",
            "[2021-01-15 13:03:39] [xla:3] Epoch [2/4], Step [90/130], Loss: 0.0861\n",
            "[2021-01-15 13:03:39] [xla:0] Epoch [2/4], Step [90/130], Loss: 0.0119\n",
            "[2021-01-15 13:03:39] [xla:4] Epoch [2/4], Step [90/130], Loss: 0.0394\n",
            "[2021-01-15 13:03:50] [xla:6] Epoch [2/4], Step [100/130], Loss: 0.0122\n",
            "[2021-01-15 13:03:50] [xla:1] Epoch [2/4], Step [100/130], Loss: 0.0155\n",
            "[2021-01-15 13:03:50] [xla:5] Epoch [2/4], Step [100/130], Loss: 0.1842\n",
            "[2021-01-15 13:03:50] [xla:7] Epoch [2/4], Step [100/130], Loss: 0.2069\n",
            "[2021-01-15 13:03:50] [xla:0] Epoch [2/4], Step [100/130], Loss: 0.0058\n",
            "[2021-01-15 13:03:50] [xla:3] Epoch [2/4], Step [100/130], Loss: 0.0327\n",
            "[2021-01-15 13:03:50] [xla:4] Epoch [2/4], Step [100/130], Loss: 0.3337\n",
            "[2021-01-15 13:03:50] [xla:2] Epoch [2/4], Step [100/130], Loss: 0.0095\n",
            "[2021-01-15 13:04:01] [xla:5] Epoch [2/4], Step [110/130], Loss: 0.1162\n",
            "[2021-01-15 13:04:01] [xla:0] Epoch [2/4], Step [110/130], Loss: 0.0645\n",
            "[2021-01-15 13:04:01] [xla:4] Epoch [2/4], Step [110/130], Loss: 0.1732\n",
            "[2021-01-15 13:04:01] [xla:7] Epoch [2/4], Step [110/130], Loss: 0.1804\n",
            "[2021-01-15 13:04:01] [xla:1] Epoch [2/4], Step [110/130], Loss: 0.0170\n",
            "[2021-01-15 13:04:01] [xla:3] Epoch [2/4], Step [110/130], Loss: 0.1046\n",
            "[2021-01-15 13:04:01] [xla:2] Epoch [2/4], Step [110/130], Loss: 0.2133\n",
            "[2021-01-15 13:04:01] [xla:6] Epoch [2/4], Step [110/130], Loss: 0.0677\n",
            "[2021-01-15 13:04:12] [xla:6] Epoch [2/4], Step [120/130], Loss: 0.0151\n",
            "[2021-01-15 13:04:12] [xla:5] Epoch [2/4], Step [120/130], Loss: 0.2318\n",
            "[2021-01-15 13:04:12] [xla:3] Epoch [2/4], Step [120/130], Loss: 0.1495\n",
            "[2021-01-15 13:04:12] [xla:4] Epoch [2/4], Step [120/130], Loss: 0.0081\n",
            "[2021-01-15 13:04:12] [xla:1] Epoch [2/4], Step [120/130], Loss: 0.0061\n",
            "[2021-01-15 13:04:12] [xla:2] Epoch [2/4], Step [120/130], Loss: 0.2478\n",
            "[2021-01-15 13:04:12] [xla:7] Epoch [2/4], Step [120/130], Loss: 0.0061\n",
            "[2021-01-15 13:04:12] [xla:0] Epoch [2/4], Step [120/130], Loss: 0.0169\n",
            "[2021-01-15 13:04:23] [xla:6] Epoch [2/4], Step [130/130], Loss: 0.0326\n",
            "[2021-01-15 13:04:23] [xla:0] Epoch [2/4], Step [130/130], Loss: 0.4903\n",
            "[2021-01-15 13:04:23] [xla:7] Epoch [2/4], Step [130/130], Loss: 0.0085\n",
            "[2021-01-15 13:04:23] [xla:4] Epoch [2/4], Step [130/130], Loss: 0.1038\n",
            "[2021-01-15 13:04:23] [xla:3] Epoch [2/4], Step [130/130], Loss: 0.1540\n",
            "[2021-01-15 13:04:23] [xla:5] Epoch [2/4], Step [130/130], Loss: 0.0068\n",
            "[2021-01-15 13:04:23] [xla:1] Epoch [2/4], Step [130/130], Loss: 0.0815\n",
            "[2021-01-15 13:04:23] [xla:2] Epoch [2/4], Step [130/130], Loss: 0.2965\n",
            "Finished training epoch 1\n",
            "[2021-01-15 13:04:48] [xla:0] samples: 3125 accuracy: 0.9312\n",
            "[2021-01-15 13:04:48] [xla:3] samples: 3125 accuracy: 0.93856\n",
            "[2021-01-15 13:04:48] [xla:2] samples: 3125 accuracy: 0.93984\n",
            "[2021-01-15 13:04:48] [xla:5] samples: 3125 accuracy: 0.94208\n",
            "[2021-01-15 13:04:48] [xla:4] samples: 3125 accuracy: 0.9408\n",
            "[2021-01-15 13:04:48] [xla:1] samples: 3125 accuracy: 0.93664\n",
            "[2021-01-15 13:04:48] [xla:7] samples: 3125 accuracy: 0.9328\n",
            "[2021-01-15 13:04:48] [xla:6] samples: 3125 accuracy: 0.94496\n",
            "reduced accuracy: 0.9383600000000001\n",
            "Finished evaluating epoch 1\n",
            "[2021-01-15 13:05:00] [xla:5] Epoch [3/4], Step [10/130], Loss: 0.0056\n",
            "[2021-01-15 13:05:00] [xla:1] Epoch [3/4], Step [10/130], Loss: 0.1858\n",
            "[2021-01-15 13:05:00] [xla:7] Epoch [3/4], Step [10/130], Loss: 0.0317\n",
            "[2021-01-15 13:05:00] [xla:3] Epoch [3/4], Step [10/130], Loss: 0.2036\n",
            "[2021-01-15 13:05:00] [xla:0] Epoch [3/4], Step [10/130], Loss: 0.0046\n",
            "[2021-01-15 13:05:00] [xla:4] Epoch [3/4], Step [10/130], Loss: 0.0031\n",
            "[2021-01-15 13:05:00] [xla:2] Epoch [3/4], Step [10/130], Loss: 0.0132\n",
            "[2021-01-15 13:05:01] [xla:6] Epoch [3/4], Step [10/130], Loss: 0.0034\n",
            "[2021-01-15 13:05:12] [xla:2] Epoch [3/4], Step [20/130], Loss: 0.0281\n",
            "[2021-01-15 13:05:12] [xla:6] Epoch [3/4], Step [20/130], Loss: 0.0061\n",
            "[2021-01-15 13:05:12] [xla:0] Epoch [3/4], Step [20/130], Loss: 0.0061\n",
            "[2021-01-15 13:05:12] [xla:4] Epoch [3/4], Step [20/130], Loss: 0.0148\n",
            "[2021-01-15 13:05:12] [xla:7] Epoch [3/4], Step [20/130], Loss: 0.0684\n",
            "[2021-01-15 13:05:12] [xla:3] Epoch [3/4], Step [20/130], Loss: 0.0596\n",
            "[2021-01-15 13:05:12] [xla:5] Epoch [3/4], Step [20/130], Loss: 0.2202\n",
            "[2021-01-15 13:05:12] [xla:1] Epoch [3/4], Step [20/130], Loss: 0.1065\n",
            "[2021-01-15 13:05:23] [xla:4] Epoch [3/4], Step [30/130], Loss: 0.0231\n",
            "[2021-01-15 13:05:23] [xla:5] Epoch [3/4], Step [30/130], Loss: 0.0049\n",
            "[2021-01-15 13:05:23] [xla:0] Epoch [3/4], Step [30/130], Loss: 0.0074\n",
            "[2021-01-15 13:05:23] [xla:7] Epoch [3/4], Step [30/130], Loss: 0.0297\n",
            "[2021-01-15 13:05:23] [xla:3] Epoch [3/4], Step [30/130], Loss: 0.2175\n",
            "[2021-01-15 13:05:23] [xla:1] Epoch [3/4], Step [30/130], Loss: 0.0043\n",
            "[2021-01-15 13:05:23] [xla:2] Epoch [3/4], Step [30/130], Loss: 0.0037\n",
            "[2021-01-15 13:05:23] [xla:6] Epoch [3/4], Step [30/130], Loss: 0.3415\n",
            "[2021-01-15 13:05:34] [xla:4] Epoch [3/4], Step [40/130], Loss: 0.2816\n",
            "[2021-01-15 13:05:34] [xla:7] Epoch [3/4], Step [40/130], Loss: 0.3378\n",
            "[2021-01-15 13:05:34] [xla:0] Epoch [3/4], Step [40/130], Loss: 0.0086\n",
            "[2021-01-15 13:05:34] [xla:3] Epoch [3/4], Step [40/130], Loss: 0.2135\n",
            "[2021-01-15 13:05:34] [xla:1] Epoch [3/4], Step [40/130], Loss: 0.0134\n",
            "[2021-01-15 13:05:34] [xla:6] Epoch [3/4], Step [40/130], Loss: 0.1200\n",
            "[2021-01-15 13:05:34] [xla:5] Epoch [3/4], Step [40/130], Loss: 0.0050\n",
            "[2021-01-15 13:05:34] [xla:2] Epoch [3/4], Step [40/130], Loss: 0.0088\n",
            "[2021-01-15 13:05:45] [xla:2] Epoch [3/4], Step [50/130], Loss: 0.0037\n",
            "[2021-01-15 13:05:45] [xla:3] Epoch [3/4], Step [50/130], Loss: 0.1549\n",
            "[2021-01-15 13:05:45] [xla:0] Epoch [3/4], Step [50/130], Loss: 0.0053\n",
            "[2021-01-15 13:05:45] [xla:6] Epoch [3/4], Step [50/130], Loss: 0.0867\n",
            "[2021-01-15 13:05:45] [xla:4] Epoch [3/4], Step [50/130], Loss: 0.1953\n",
            "[2021-01-15 13:05:45] [xla:1] Epoch [3/4], Step [50/130], Loss: 0.0034\n",
            "[2021-01-15 13:05:45] [xla:5] Epoch [3/4], Step [50/130], Loss: 0.0042\n",
            "[2021-01-15 13:05:45] [xla:7] Epoch [3/4], Step [50/130], Loss: 0.0035\n",
            "[2021-01-15 13:05:56] [xla:4] Epoch [3/4], Step [60/130], Loss: 0.0044\n",
            "[2021-01-15 13:05:56] [xla:1] Epoch [3/4], Step [60/130], Loss: 0.4176\n",
            "[2021-01-15 13:05:56] [xla:7] Epoch [3/4], Step [60/130], Loss: 0.2338\n",
            "[2021-01-15 13:05:56] [xla:3] Epoch [3/4], Step [60/130], Loss: 0.5468\n",
            "[2021-01-15 13:05:56] [xla:5] Epoch [3/4], Step [60/130], Loss: 0.2694\n",
            "[2021-01-15 13:05:56] [xla:0] Epoch [3/4], Step [60/130], Loss: 0.0604\n",
            "[2021-01-15 13:05:56] [xla:6] Epoch [3/4], Step [60/130], Loss: 0.0038\n",
            "[2021-01-15 13:05:56] [xla:2] Epoch [3/4], Step [60/130], Loss: 0.3685\n",
            "[2021-01-15 13:06:07] [xla:3] Epoch [3/4], Step [70/130], Loss: 0.0377\n",
            "[2021-01-15 13:06:07] [xla:1] Epoch [3/4], Step [70/130], Loss: 0.0620\n",
            "[2021-01-15 13:06:07] [xla:2] Epoch [3/4], Step [70/130], Loss: 0.0067\n",
            "[2021-01-15 13:06:07] [xla:5] Epoch [3/4], Step [70/130], Loss: 0.1152\n",
            "[2021-01-15 13:06:07] [xla:4] Epoch [3/4], Step [70/130], Loss: 0.0054\n",
            "[2021-01-15 13:06:07] [xla:7] Epoch [3/4], Step [70/130], Loss: 0.0188\n",
            "[2021-01-15 13:06:07] [xla:0] Epoch [3/4], Step [70/130], Loss: 0.3509\n",
            "[2021-01-15 13:06:07] [xla:6] Epoch [3/4], Step [70/130], Loss: 0.0040\n",
            "[2021-01-15 13:06:18] [xla:2] Epoch [3/4], Step [80/130], Loss: 0.0223\n",
            "[2021-01-15 13:06:18] [xla:3] Epoch [3/4], Step [80/130], Loss: 0.2161\n",
            "[2021-01-15 13:06:18] [xla:5] Epoch [3/4], Step [80/130], Loss: 0.0757\n",
            "[2021-01-15 13:06:18] [xla:7] Epoch [3/4], Step [80/130], Loss: 0.0028\n",
            "[2021-01-15 13:06:18] [xla:6] Epoch [3/4], Step [80/130], Loss: 0.0051\n",
            "[2021-01-15 13:06:18] [xla:0] Epoch [3/4], Step [80/130], Loss: 0.2653\n",
            "[2021-01-15 13:06:18] [xla:1] Epoch [3/4], Step [80/130], Loss: 0.2400\n",
            "[2021-01-15 13:06:18] [xla:4] Epoch [3/4], Step [80/130], Loss: 0.0091\n",
            "[2021-01-15 13:06:28] [xla:6] Epoch [3/4], Step [90/130], Loss: 0.4706\n",
            "[2021-01-15 13:06:29] [xla:3] Epoch [3/4], Step [90/130], Loss: 0.0053\n",
            "[2021-01-15 13:06:29] [xla:1] Epoch [3/4], Step [90/130], Loss: 0.0033\n",
            "[2021-01-15 13:06:29] [xla:7] Epoch [3/4], Step [90/130], Loss: 0.0070\n",
            "[2021-01-15 13:06:29] [xla:5] Epoch [3/4], Step [90/130], Loss: 0.0045\n",
            "[2021-01-15 13:06:29] [xla:2] Epoch [3/4], Step [90/130], Loss: 0.2224\n",
            "[2021-01-15 13:06:29] [xla:0] Epoch [3/4], Step [90/130], Loss: 0.0028\n",
            "[2021-01-15 13:06:29] [xla:4] Epoch [3/4], Step [90/130], Loss: 0.0038\n",
            "[2021-01-15 13:06:40] [xla:7] Epoch [3/4], Step [100/130], Loss: 0.0535\n",
            "[2021-01-15 13:06:40] [xla:3] Epoch [3/4], Step [100/130], Loss: 0.0049\n",
            "[2021-01-15 13:06:40] [xla:5] Epoch [3/4], Step [100/130], Loss: 0.0037\n",
            "[2021-01-15 13:06:40] [xla:1] Epoch [3/4], Step [100/130], Loss: 0.0033\n",
            "[2021-01-15 13:06:40] [xla:6] Epoch [3/4], Step [100/130], Loss: 0.0087\n",
            "[2021-01-15 13:06:40] [xla:4] Epoch [3/4], Step [100/130], Loss: 0.2634\n",
            "[2021-01-15 13:06:40] [xla:0] Epoch [3/4], Step [100/130], Loss: 0.0029\n",
            "[2021-01-15 13:06:40] [xla:2] Epoch [3/4], Step [100/130], Loss: 0.0030\n",
            "[2021-01-15 13:06:51] [xla:6] Epoch [3/4], Step [110/130], Loss: 0.1215\n",
            "[2021-01-15 13:06:51] [xla:7] Epoch [3/4], Step [110/130], Loss: 0.0030\n",
            "[2021-01-15 13:06:51] [xla:0] Epoch [3/4], Step [110/130], Loss: 0.0050\n",
            "[2021-01-15 13:06:51] [xla:5] Epoch [3/4], Step [110/130], Loss: 0.0283\n",
            "[2021-01-15 13:06:51] [xla:3] Epoch [3/4], Step [110/130], Loss: 0.0025\n",
            "[2021-01-15 13:06:51] [xla:1] Epoch [3/4], Step [110/130], Loss: 0.0025\n",
            "[2021-01-15 13:06:51] [xla:2] Epoch [3/4], Step [110/130], Loss: 0.2052\n",
            "[2021-01-15 13:06:51] [xla:4] Epoch [3/4], Step [110/130], Loss: 0.0102\n",
            "[2021-01-15 13:07:02] [xla:2] Epoch [3/4], Step [120/130], Loss: 0.0153\n",
            "[2021-01-15 13:07:02] [xla:1] Epoch [3/4], Step [120/130], Loss: 0.0026\n",
            "[2021-01-15 13:07:02] [xla:6] Epoch [3/4], Step [120/130], Loss: 0.0333\n",
            "[2021-01-15 13:07:02] [xla:5] Epoch [3/4], Step [120/130], Loss: 0.2274\n",
            "[2021-01-15 13:07:02] [xla:7] Epoch [3/4], Step [120/130], Loss: 0.0025\n",
            "[2021-01-15 13:07:02] [xla:4] Epoch [3/4], Step [120/130], Loss: 0.0036\n",
            "[2021-01-15 13:07:02] [xla:0] Epoch [3/4], Step [120/130], Loss: 0.0025\n",
            "[2021-01-15 13:07:02] [xla:3] Epoch [3/4], Step [120/130], Loss: 0.2160\n",
            "[2021-01-15 13:07:13] [xla:5] Epoch [3/4], Step [130/130], Loss: 0.0026\n",
            "[2021-01-15 13:07:13] [xla:3] Epoch [3/4], Step [130/130], Loss: 0.0178\n",
            "[2021-01-15 13:07:13] [xla:6] Epoch [3/4], Step [130/130], Loss: 0.2091\n",
            "[2021-01-15 13:07:13] [xla:1] Epoch [3/4], Step [130/130], Loss: 0.0030\n",
            "[2021-01-15 13:07:13] [xla:0] Epoch [3/4], Step [130/130], Loss: 0.2650\n",
            "[2021-01-15 13:07:13] [xla:4] Epoch [3/4], Step [130/130], Loss: 0.0139\n",
            "[2021-01-15 13:07:13] [xla:7] Epoch [3/4], Step [130/130], Loss: 0.0963\n",
            "[2021-01-15 13:07:13] [xla:2] Epoch [3/4], Step [130/130], Loss: 0.2698\n",
            "Finished training epoch 2\n",
            "[2021-01-15 13:07:39] [xla:3] samples: 3125 accuracy: 0.93984\n",
            "[2021-01-15 13:07:39] [xla:5] samples: 3125 accuracy: 0.93984\n",
            "[2021-01-15 13:07:39] [xla:6] samples: 3125 accuracy: 0.94368\n",
            "[2021-01-15 13:07:39] [xla:2] samples: 3125 accuracy: 0.9392\n",
            "[2021-01-15 13:07:39] [xla:1] samples: 3125 accuracy: 0.9376\n",
            "[2021-01-15 13:07:39] [xla:0] samples: 3125 accuracy: 0.92928\n",
            "[2021-01-15 13:07:39] [xla:7] samples: 3125 accuracy: 0.93888\n",
            "[2021-01-15 13:07:39] [xla:4] samples: 3125 accuracy: 0.94176\n",
            "reduced accuracy: 0.93876\n",
            "Finished evaluating epoch 2\n",
            "[2021-01-15 13:07:52] [xla:0] Epoch [4/4], Step [10/130], Loss: 0.0026\n",
            "[2021-01-15 13:07:52] [xla:4] Epoch [4/4], Step [10/130], Loss: 0.0021\n",
            "[2021-01-15 13:07:52] [xla:5] Epoch [4/4], Step [10/130], Loss: 0.0024\n",
            "[2021-01-15 13:07:52] [xla:7] Epoch [4/4], Step [10/130], Loss: 0.0047\n",
            "[2021-01-15 13:07:52] [xla:1] Epoch [4/4], Step [10/130], Loss: 0.0455\n",
            "[2021-01-15 13:07:52] [xla:3] Epoch [4/4], Step [10/130], Loss: 0.0058\n",
            "[2021-01-15 13:07:52] [xla:2] Epoch [4/4], Step [10/130], Loss: 0.0042\n",
            "[2021-01-15 13:07:52] [xla:6] Epoch [4/4], Step [10/130], Loss: 0.0020\n",
            "[2021-01-15 13:08:03] [xla:0] Epoch [4/4], Step [20/130], Loss: 0.0027\n",
            "[2021-01-15 13:08:03] [xla:3] Epoch [4/4], Step [20/130], Loss: 0.0028\n",
            "[2021-01-15 13:08:03] [xla:6] Epoch [4/4], Step [20/130], Loss: 0.0028\n",
            "[2021-01-15 13:08:03] [xla:5] Epoch [4/4], Step [20/130], Loss: 0.0042\n",
            "[2021-01-15 13:08:03] [xla:1] Epoch [4/4], Step [20/130], Loss: 0.0034\n",
            "[2021-01-15 13:08:03] [xla:2] Epoch [4/4], Step [20/130], Loss: 0.0042\n",
            "[2021-01-15 13:08:03] [xla:4] Epoch [4/4], Step [20/130], Loss: 0.0035\n",
            "[2021-01-15 13:08:03] [xla:7] Epoch [4/4], Step [20/130], Loss: 0.0122\n",
            "[2021-01-15 13:08:15] [xla:1] Epoch [4/4], Step [30/130], Loss: 0.0022\n",
            "[2021-01-15 13:08:15] [xla:3] Epoch [4/4], Step [30/130], Loss: 0.0763\n",
            "[2021-01-15 13:08:15] [xla:0] Epoch [4/4], Step [30/130], Loss: 0.0027\n",
            "[2021-01-15 13:08:15] [xla:7] Epoch [4/4], Step [30/130], Loss: 0.0033\n",
            "[2021-01-15 13:08:15] [xla:5] Epoch [4/4], Step [30/130], Loss: 0.0024\n",
            "[2021-01-15 13:08:15] [xla:4] Epoch [4/4], Step [30/130], Loss: 0.0058\n",
            "[2021-01-15 13:08:15] [xla:2] Epoch [4/4], Step [30/130], Loss: 0.0027\n",
            "[2021-01-15 13:08:15] [xla:6] Epoch [4/4], Step [30/130], Loss: 0.1376\n",
            "[2021-01-15 13:08:26] [xla:7] Epoch [4/4], Step [40/130], Loss: 0.3298\n",
            "[2021-01-15 13:08:26] [xla:2] Epoch [4/4], Step [40/130], Loss: 0.0028\n",
            "[2021-01-15 13:08:26] [xla:6] Epoch [4/4], Step [40/130], Loss: 0.0718\n",
            "[2021-01-15 13:08:26] [xla:3] Epoch [4/4], Step [40/130], Loss: 0.1492\n",
            "[2021-01-15 13:08:26] [xla:1] Epoch [4/4], Step [40/130], Loss: 0.0039\n",
            "[2021-01-15 13:08:26] [xla:0] Epoch [4/4], Step [40/130], Loss: 0.0025\n",
            "[2021-01-15 13:08:26] [xla:4] Epoch [4/4], Step [40/130], Loss: 0.2721\n",
            "[2021-01-15 13:08:26] [xla:5] Epoch [4/4], Step [40/130], Loss: 0.0028\n",
            "[2021-01-15 13:08:37] [xla:6] Epoch [4/4], Step [50/130], Loss: 0.0031\n",
            "[2021-01-15 13:08:37] [xla:0] Epoch [4/4], Step [50/130], Loss: 0.0044\n",
            "[2021-01-15 13:08:37] [xla:3] Epoch [4/4], Step [50/130], Loss: 0.0597\n",
            "[2021-01-15 13:08:37] [xla:4] Epoch [4/4], Step [50/130], Loss: 0.1814\n",
            "[2021-01-15 13:08:37] [xla:7] Epoch [4/4], Step [50/130], Loss: 0.0023\n",
            "[2021-01-15 13:08:37] [xla:5] Epoch [4/4], Step [50/130], Loss: 0.0034\n",
            "[2021-01-15 13:08:37] [xla:2] Epoch [4/4], Step [50/130], Loss: 0.0024\n",
            "[2021-01-15 13:08:37] [xla:1] Epoch [4/4], Step [50/130], Loss: 0.0021\n",
            "[2021-01-15 13:08:49] [xla:1] Epoch [4/4], Step [60/130], Loss: 0.2795\n",
            "[2021-01-15 13:08:49] [xla:2] Epoch [4/4], Step [60/130], Loss: 0.2228\n",
            "[2021-01-15 13:08:49] [xla:3] Epoch [4/4], Step [60/130], Loss: 0.2390\n",
            "[2021-01-15 13:08:49] [xla:7] Epoch [4/4], Step [60/130], Loss: 0.0067\n",
            "[2021-01-15 13:08:49] [xla:5] Epoch [4/4], Step [60/130], Loss: 0.2595\n",
            "[2021-01-15 13:08:49] [xla:4] Epoch [4/4], Step [60/130], Loss: 0.0033\n",
            "[2021-01-15 13:08:49] [xla:0] Epoch [4/4], Step [60/130], Loss: 0.0367\n",
            "[2021-01-15 13:08:49] [xla:6] Epoch [4/4], Step [60/130], Loss: 0.0025\n",
            "[2021-01-15 13:09:00] [xla:7] Epoch [4/4], Step [70/130], Loss: 0.0029\n",
            "[2021-01-15 13:09:00] [xla:6] Epoch [4/4], Step [70/130], Loss: 0.0030\n",
            "[2021-01-15 13:09:00] [xla:4] Epoch [4/4], Step [70/130], Loss: 0.0036\n",
            "[2021-01-15 13:09:00] [xla:3] Epoch [4/4], Step [70/130], Loss: 0.0739\n",
            "[2021-01-15 13:09:00] [xla:1] Epoch [4/4], Step [70/130], Loss: 0.0064\n",
            "[2021-01-15 13:09:00] [xla:2] Epoch [4/4], Step [70/130], Loss: 0.0027\n",
            "[2021-01-15 13:09:00] [xla:0] Epoch [4/4], Step [70/130], Loss: 0.3442\n",
            "[2021-01-15 13:09:00] [xla:5] Epoch [4/4], Step [70/130], Loss: 0.0918\n",
            "[2021-01-15 13:09:12] [xla:6] Epoch [4/4], Step [80/130], Loss: 0.0694\n",
            "[2021-01-15 13:09:12] [xla:2] Epoch [4/4], Step [80/130], Loss: 0.0025\n",
            "[2021-01-15 13:09:12] [xla:3] Epoch [4/4], Step [80/130], Loss: 0.2419\n",
            "[2021-01-15 13:09:12] [xla:1] Epoch [4/4], Step [80/130], Loss: 0.0425\n",
            "[2021-01-15 13:09:12] [xla:5] Epoch [4/4], Step [80/130], Loss: 0.0027\n",
            "[2021-01-15 13:09:12] [xla:0] Epoch [4/4], Step [80/130], Loss: 0.2692\n",
            "[2021-01-15 13:09:12] [xla:7] Epoch [4/4], Step [80/130], Loss: 0.0020\n",
            "[2021-01-15 13:09:12] [xla:4] Epoch [4/4], Step [80/130], Loss: 0.0053\n",
            "[2021-01-15 13:09:23] [xla:5] Epoch [4/4], Step [90/130], Loss: 0.0024\n",
            "[2021-01-15 13:09:23] [xla:6] Epoch [4/4], Step [90/130], Loss: 0.4546\n",
            "[2021-01-15 13:09:23] [xla:1] Epoch [4/4], Step [90/130], Loss: 0.0026\n",
            "[2021-01-15 13:09:23] [xla:4] Epoch [4/4], Step [90/130], Loss: 0.0024\n",
            "[2021-01-15 13:09:23] [xla:2] Epoch [4/4], Step [90/130], Loss: 0.2305\n",
            "[2021-01-15 13:09:23] [xla:0] Epoch [4/4], Step [90/130], Loss: 0.0020\n",
            "[2021-01-15 13:09:23] [xla:7] Epoch [4/4], Step [90/130], Loss: 0.0033\n",
            "[2021-01-15 13:09:23] [xla:3] Epoch [4/4], Step [90/130], Loss: 0.0119\n",
            "[2021-01-15 13:09:34] [xla:6] Epoch [4/4], Step [100/130], Loss: 0.0033\n",
            "[2021-01-15 13:09:34] [xla:3] Epoch [4/4], Step [100/130], Loss: 0.0034\n",
            "[2021-01-15 13:09:34] [xla:1] Epoch [4/4], Step [100/130], Loss: 0.0045\n",
            "[2021-01-15 13:09:34] [xla:4] Epoch [4/4], Step [100/130], Loss: 0.2692\n",
            "[2021-01-15 13:09:35] [xla:7] Epoch [4/4], Step [100/130], Loss: 0.0035\n",
            "[2021-01-15 13:09:34] [xla:5] Epoch [4/4], Step [100/130], Loss: 0.0039\n",
            "[2021-01-15 13:09:35] [xla:0] Epoch [4/4], Step [100/130], Loss: 0.0024\n",
            "[2021-01-15 13:09:35] [xla:2] Epoch [4/4], Step [100/130], Loss: 0.0027\n",
            "[2021-01-15 13:09:46] [xla:4] Epoch [4/4], Step [110/130], Loss: 0.0054\n",
            "[2021-01-15 13:09:46] [xla:1] Epoch [4/4], Step [110/130], Loss: 0.0031\n",
            "[2021-01-15 13:09:46] [xla:5] Epoch [4/4], Step [110/130], Loss: 0.0052\n",
            "[2021-01-15 13:09:46] [xla:6] Epoch [4/4], Step [110/130], Loss: 0.0050\n",
            "[2021-01-15 13:09:46] [xla:7] Epoch [4/4], Step [110/130], Loss: 0.0035\n",
            "[2021-01-15 13:09:46] [xla:0] Epoch [4/4], Step [110/130], Loss: 0.0056\n",
            "[2021-01-15 13:09:46] [xla:2] Epoch [4/4], Step [110/130], Loss: 0.0042\n",
            "[2021-01-15 13:09:46] [xla:3] Epoch [4/4], Step [110/130], Loss: 0.0031\n",
            "[2021-01-15 13:09:57] [xla:5] Epoch [4/4], Step [120/130], Loss: 0.1320\n",
            "[2021-01-15 13:09:57] [xla:3] Epoch [4/4], Step [120/130], Loss: 0.0065\n",
            "[2021-01-15 13:09:57] [xla:7] Epoch [4/4], Step [120/130], Loss: 0.0026\n",
            "[2021-01-15 13:09:57] [xla:4] Epoch [4/4], Step [120/130], Loss: 0.0029\n",
            "[2021-01-15 13:09:57] [xla:6] Epoch [4/4], Step [120/130], Loss: 0.0030\n",
            "[2021-01-15 13:09:57] [xla:2] Epoch [4/4], Step [120/130], Loss: 0.0061\n",
            "[2021-01-15 13:09:57] [xla:0] Epoch [4/4], Step [120/130], Loss: 0.0034\n",
            "[2021-01-15 13:09:57] [xla:1] Epoch [4/4], Step [120/130], Loss: 0.0024\n",
            "[2021-01-15 13:10:08] [xla:2] Epoch [4/4], Step [130/130], Loss: 0.1390\n",
            "[2021-01-15 13:10:08] [xla:0] Epoch [4/4], Step [130/130], Loss: 0.2370\n",
            "[2021-01-15 13:10:08] [xla:6] Epoch [4/4], Step [130/130], Loss: 0.0067\n",
            "[2021-01-15 13:10:08] [xla:7] Epoch [4/4], Step [130/130], Loss: 0.0032\n",
            "Finished training epoch 3\n",
            "[2021-01-15 13:10:08] [xla:1] Epoch [4/4], Step [130/130], Loss: 0.0029\n",
            "[2021-01-15 13:10:08] [xla:3] Epoch [4/4], Step [130/130], Loss: 0.0029\n",
            "[2021-01-15 13:10:08] [xla:4] Epoch [4/4], Step [130/130], Loss: 0.0033\n",
            "[2021-01-15 13:10:08] [xla:5] Epoch [4/4], Step [130/130], Loss: 0.0029\n",
            "[2021-01-15 13:10:33] [xla:0] samples: 3125 accuracy: 0.93152\n",
            "[2021-01-15 13:10:34] [xla:2] samples: 3125 accuracy: 0.94304\n",
            "[2021-01-15 13:10:34] [xla:7] samples: 3125 accuracy: 0.93792\n",
            "[2021-01-15 13:10:34] [xla:6] samples: 3125 accuracy: 0.94592\n",
            "[2021-01-15 13:10:34] [xla:3] samples: 3125 accuracy: 0.944\n",
            "[2021-01-15 13:10:34] [xla:5] samples: 3125 accuracy: 0.94176\n",
            "[2021-01-15 13:10:34] [xla:4] samples: 3125 accuracy: 0.94336\n",
            "[2021-01-15 13:10:34] [xla:1] samples: 3125 accuracy: 0.94016\n",
            "reduced accuracy: 0.94096\n",
            "Finished evaluating epoch 3\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}